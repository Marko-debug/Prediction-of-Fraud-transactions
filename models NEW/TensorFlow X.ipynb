{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e56bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import numpy as np\n",
    "from sklearn.utils import class_weight\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b413e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Load and Merge Data ---\n",
    "\n",
    "# Load data\n",
    "train_transaction = pd.read_csv('C:/Users/Marek/Desktop/2024-2025/Diplomov√° pr√°ca/Diplomov√° pr√°ca/Prediction-of-Fraud-transactions/input/train_transaction.csv')\n",
    "train_identity = pd.read_csv('C:/Users/Marek/Desktop/2024-2025/Diplomov√° pr√°ca/Diplomov√° pr√°ca/Prediction-of-Fraud-transactions/input/train_identity.csv')\n",
    "\n",
    "# Merge datasets\n",
    "train_df = pd.merge(train_transaction, train_identity, on='TransactionID', how='left')\n",
    "\n",
    "# Simple target definition\n",
    "# X_check = train_df.drop(['isFraud', 'TransactionID', 'TransactionDT'], axis=1)\n",
    "# y_check = train_df['isFraud']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb25e94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.sort_values('TransactionDT')\n",
    "\n",
    "# 2. Calculate Cutoff Points\n",
    "n_total = len(train_df)\n",
    "idx_train_end = int(n_total * 0.70)      # End of first 70%\n",
    "idx_val_end   = int(n_total * 0.85)      # End of next 15% (70+15=85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb702ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Slice the Dataframe\n",
    "# .iloc works by position [start : end]\n",
    "train_set = train_df.iloc[ :idx_train_end]              # 0% to 70%\n",
    "val_set   = train_df.iloc[idx_train_end : idx_val_end]  # 70% to 85%\n",
    "test_set  = train_df.iloc[idx_val_end: ]                # 85% to 100%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dac0044",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['isFraud', 'TransactionID', 'TransactionDT']\n",
    "\n",
    "# TRAIN\n",
    "X_train = train_set.drop(cols_to_drop, axis=1)\n",
    "y_train = train_set['isFraud']\n",
    "\n",
    "# VALIDATION\n",
    "X_val = val_set.drop(cols_to_drop, axis=1)\n",
    "y_val = val_set['isFraud']\n",
    "\n",
    "# TEST (The Hold-out)\n",
    "X_test = test_set.drop(cols_to_drop, axis=1)\n",
    "y_test = test_set['isFraud']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f6b5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_threshold = 0.80\n",
    "\n",
    "missing_series = X_train.isnull().mean()\n",
    "drop_cols = missing_series[missing_series > missing_threshold].index\n",
    "print(f\"Dropping {len(drop_cols)} columns with > {missing_threshold:.0%} missing values.\")\n",
    "\n",
    "X_train_clean = X_train.drop(columns=drop_cols)\n",
    "X_val_clean   = X_val.drop(columns=drop_cols)\n",
    "X_test_clean  = X_test.drop(columns=drop_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba2ba80",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_xgb = X_train_clean.copy()\n",
    "\n",
    "print(\"Encoding categorical columns for XGBoost...\")\n",
    "for col in X_xgb.select_dtypes(include=['object']).columns:\n",
    "    X_xgb[col] = X_xgb[col].astype('category').cat.codes\n",
    "\n",
    "# 2. Configure & Train XGBoost\n",
    "# We use 'hist' tree method because it's extremely fast\n",
    "clf = XGBClassifier(\n",
    "    n_estimators=100,       # 100 trees is enough for feature selection\n",
    "    max_depth=10,           # Deep trees to find complex fraud patterns\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    missing=np.nan,         # Critical: Tells XGBoost to handle NaNs automatically\n",
    "    n_jobs=-1,              # Use all CPU cores\n",
    "    random_state=42,\n",
    "    tree_method='hist'      # Fast training mode\n",
    ")\n",
    "\n",
    "print(\"Training XGBoost to find the most important features...\")\n",
    "clf.fit(X_xgb, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571e0f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = pd.Series(clf.get_booster().get_score(importance_type='total_gain'))\n",
    "\n",
    "# 2. Sort and Take Top 50\n",
    "TOP_N = 50\n",
    "best_features = importances.sort_values(ascending=False).head(TOP_N).index.tolist()\n",
    "\n",
    "print(f\"--- Top {TOP_N} Features Selected ---\")\n",
    "print(best_features[:10])  # Print top 10 to check\n",
    "\n",
    "# 3. Visualize\n",
    "plt.figure(figsize=(10, 8))\n",
    "importances.sort_values(ascending=False).head(20).plot(kind='barh', color='teal')\n",
    "plt.title('Top 20 Features by XGBoost Importance')\n",
    "plt.gca().invert_yaxis() # Highest importance on top\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd6cff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL_CAT_COLS = []\n",
    "FINAL_NUM_COLS = []\n",
    "\n",
    "print(\"classifying Top Features...\")\n",
    "\n",
    "for col in best_features:\n",
    "    # A. Is it an Object (String) in the original cleaned data?\n",
    "    if X_train_clean[col].dtype == 'object':\n",
    "        FINAL_CAT_COLS.append(col)\n",
    "        \n",
    "    # B. Is it a Number but behaves like a Category? (Few unique values)\n",
    "    # Example: 'card4' might be 1, 2, 3, 4. Treat as Category.\n",
    "    elif X_train_clean[col].nunique() < 20:\n",
    "        FINAL_CAT_COLS.append(col)\n",
    "        \n",
    "    # C. Otherwise, it is a Number (Amount, Time, Distance)\n",
    "    else:\n",
    "        FINAL_NUM_COLS.append(col)\n",
    "\n",
    "print(f\"--------------------------------\")\n",
    "print(f\"Feature Split for Neural Network:\")\n",
    "print(f\" - Categorical (Embeddings): {len(FINAL_CAT_COLS)}\")\n",
    "print(f\" - Numerical (Scaled):       {len(FINAL_NUM_COLS)}\")\n",
    "print(f\"--------------------------------\")\n",
    "\n",
    "# Finalize the Datasets\n",
    "# We now discard all other columns and keep only the winners\n",
    "X_train_ready = X_train_clean[best_features].copy()\n",
    "X_val_ready   = X_val_clean[best_features].copy()\n",
    "X_test_ready  = X_test_clean[best_features].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc47a2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. COMBINE TEMPORARILY\n",
    "# We need to make sure \"Visa\" gets the same ID (e.g., 5) in both Train and Test.\n",
    "all_data = pd.concat([X_train_ready, X_val_ready, X_test_ready])\n",
    "\n",
    "print(f\"Processing {all_data.shape[1]} features for {len(all_data)} rows...\")\n",
    "\n",
    "# 2. PROCESS CATEGORICAL COLUMNS (For Embeddings)\n",
    "print(\"Encoding Categorical Features...\")\n",
    "for col in FINAL_CAT_COLS:\n",
    "    # A. Force to String and Fill Missing\n",
    "    all_data[col] = all_data[col].astype(str).fillna(\"Missing\")\n",
    "    \n",
    "    # B. Label Encode (String -> Integer)\n",
    "    le = LabelEncoder()\n",
    "    all_data[col] = le.fit_transform(all_data[col])\n",
    "    \n",
    "    # C. Optimize Memory\n",
    "    all_data[col] = all_data[col].astype('int32')\n",
    "\n",
    "# 3. PROCESS NUMERICAL COLUMNS (For Scaling)\n",
    "print(\"Scaling Numerical Features...\")\n",
    "\n",
    "# A. Fill Missing with MEAN (Average) instead of 0\n",
    "# This prevents the model from thinking missing data equals \"0.0 distance\" or \"0.0 money\"\n",
    "for col in FINAL_NUM_COLS:\n",
    "    if col in all_data.columns:\n",
    "        # Calculate average (ignoring NaNs)\n",
    "        mean_val = all_data[col].mean()\n",
    "            \n",
    "        # Fill missing values\n",
    "        all_data[col] = all_data[col].fillna(mean_val)\n",
    "\n",
    "# B. Scale (Standardize to mean 0, std 1)\n",
    "# Because we filled with mean, the missing values will technically become 0.0 \n",
    "# AFTER this scaling step, which is the perfect \"neutral\" signal for a Neural Net.\n",
    "scaler = StandardScaler()\n",
    "all_data[FINAL_NUM_COLS] = scaler.fit_transform(all_data[FINAL_NUM_COLS])\n",
    "all_data[FINAL_NUM_COLS] = all_data[FINAL_NUM_COLS].astype('float32')\n",
    "# --- CHANGED SECTION END ---\n",
    "\n",
    "# 4. SPLIT BACK APART\n",
    "# We use the lengths of your original sets to cut the big dataframe\n",
    "len_train = len(X_train_ready)\n",
    "len_val   = len(X_val_ready)\n",
    "\n",
    "train_df_final = all_data.iloc[:len_train]\n",
    "val_df_final   = all_data.iloc[len_train : len_train + len_val]\n",
    "test_df_final  = all_data.iloc[len_train + len_val :]\n",
    "\n",
    "# 5. CREATE DICTIONARIES (The TensorFlow Input Format)\n",
    "# The model expects: {'feature_name': [values], ...}\n",
    "def make_inputs(df):\n",
    "    return {col: df[col].values for col in df.columns}\n",
    "\n",
    "train_inputs = make_inputs(train_df_final)\n",
    "val_inputs   = make_inputs(val_df_final)\n",
    "test_inputs  = make_inputs(test_df_final)\n",
    "\n",
    "print(\"‚úÖ Data is fully prepped and ready for the Model!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7063629d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "\n",
    "# Calculate how much weight to give to Fraud (1) vs Safe (0)\n",
    "weights = class_weight.compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "\n",
    "# Convert to dictionary {0: 0.5, 1: 14.0}\n",
    "# This tells the model: \"Every Fraud mistake is 14x worse than a Safe mistake.\"\n",
    "class_weights = {0: weights[0], 1: weights[1]}\n",
    "\n",
    "print(f\"Class Weights: {class_weights}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea7a31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_df, cat_cols, num_cols):\n",
    "    inputs = {}\n",
    "    encoded_features = []\n",
    "    \n",
    "    # --- A. CATEGORICAL INPUTS (Embeddings) ---\n",
    "    for col in cat_cols:\n",
    "        # 1. Input Layer (Receives the Integer ID)\n",
    "        input_layer = layers.Input(shape=(1,), name=col)\n",
    "        inputs[col] = input_layer\n",
    "        \n",
    "        # 2. Define Dictionary Size (Max ID + Buffer)\n",
    "        vocab_size = int(input_df[col].max()) + 5\n",
    "        \n",
    "        # 3. Define Vector Size (Rule of thumb: half of vocab, max 50)\n",
    "        embed_dim = min(50, (vocab_size + 1) // 2)\n",
    "        \n",
    "        # 4. Create Embedding (The Lookup Table)\n",
    "        emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)(input_layer)\n",
    "        emb = layers.Flatten()(emb)\n",
    "        encoded_features.append(emb)\n",
    "        \n",
    "    # --- B. NUMERICAL INPUTS (Direct) ---\n",
    "    for col in num_cols:\n",
    "        # 1. Input Layer (Receives the Scaled Number)\n",
    "        input_layer = layers.Input(shape=(1,), name=col)\n",
    "        inputs[col] = input_layer\n",
    "        \n",
    "        # 2. Add straight to the pile\n",
    "        encoded_features.append(input_layer)\n",
    "        \n",
    "    # --- C. THE BRAIN (Deep Layers) ---\n",
    "    # 1. Merge all inputs into one long row\n",
    "    all_features = layers.concatenate(encoded_features)\n",
    "    \n",
    "    # 2. First Thinking Layer (256 Neurons)\n",
    "    x = layers.Dense(256, activation='relu')(all_features)\n",
    "    x = layers.BatchNormalization()(x) # Stabilize math\n",
    "    x = layers.Dropout(0.3)(x)         # Prevent memorization\n",
    "    \n",
    "    # 3. Second Thinking Layer (128 Neurons)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "\n",
    "    # 4. Third Thinking Layer (64 Neurons)\n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    \n",
    "    # 5. Output Layer (0.0 to 1.0 probability)\n",
    "    output = layers.Dense(1, activation='sigmoid', name='fraud_prob')(x)\n",
    "    \n",
    "    model = models.Model(inputs=inputs, outputs=output)\n",
    "    return model\n",
    "\n",
    "# BUILD IT\n",
    "# We pass train_df_final to calculate the vocab sizes\n",
    "model = build_model(train_df_final, FINAL_CAT_COLS, FINAL_NUM_COLS)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f4d7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. COMPILE (The Settings)\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='binary_crossentropy', \n",
    "    # This is the critical line: Calculate ROC-AUC live\n",
    "    metrics=[tf.keras.metrics.AUC(name='auc')]\n",
    ")\n",
    "\n",
    "# 2. DEFINE CALLBACKS (The \"Managers\")\n",
    "callbacks_list = [\n",
    "    # A. The Sniper: Saves the model ONLY when AUC hits a new record high\n",
    "    callbacks.ModelCheckpoint(\n",
    "        filepath='best_fraud_model.keras',\n",
    "        monitor='val_auc',\n",
    "        mode='max',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    # B. The Safety Net: Stops if we stop improving, and RESTORES the best version\n",
    "    callbacks.EarlyStopping(\n",
    "        monitor='val_auc',\n",
    "        mode='max',\n",
    "        patience=5,               # Wait 5 epochs to be sure\n",
    "        restore_best_weights=True, # CRITICAL: Reverts model to the best epoch\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    # C. The Teacher: Lowers learning rate if the model gets stuck\n",
    "    callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_auc',\n",
    "        mode='max',\n",
    "        factor=0.5,\n",
    "        patience=2,\n",
    "        min_lr=0.00001,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# 3. TRAIN\n",
    "print(\"üöÄ Starting Training to Maximize ROC-AUC...\")\n",
    "print(\"The model will automatically save the version with the highest Validation AUC.\")\n",
    "\n",
    "history = model.fit(\n",
    "    train_inputs, y_train,\n",
    "    validation_data=(val_inputs, y_val),\n",
    "    epochs=50,                  # Give it plenty of time (it will stop early anyway)\n",
    "    batch_size=2048,            # Large batch size helps stability\n",
    "    class_weight=class_weights, # Use the weights we calculated earlier\n",
    "    callbacks=callbacks_list,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97115a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "print(\"------------------------------------------------\")\n",
    "print(\"Evaluating Best Model on TEST Set...\")\n",
    "\n",
    "# 1. Get predictions (Probabilities between 0 and 1)\n",
    "test_preds = model.predict(test_inputs, batch_size=2048).flatten()\n",
    "\n",
    "# 2. Calculate the Final ROC-AUC\n",
    "final_auc = roc_auc_score(y_test, test_preds)\n",
    "\n",
    "print(f\"üèÜ FINAL TEST ROC-AUC: {final_auc:.5f}\")\n",
    "print(\"------------------------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
